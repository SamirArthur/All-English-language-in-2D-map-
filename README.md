##Word2Vec-to-Cluster-English

What does English language look like if we print it on a 2D map? here is a clustering of 300k words on top on SOM applied to Word2Vec embeddings.

- Embeddings used: Google's Word2Vec model, source: https://code.google.com/archive/p/word2vec/ and SLIM version: https://github.com/eyaler/word2vec-slim

- SOM: Self-Organised Maps from Sompy package

- Clustering: k-means on top of the SOM map

Results: have a look at the inforgrafic in this repository :) 
